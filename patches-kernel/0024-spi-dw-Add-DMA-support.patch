From 6b844bc3d7216f8579f726056833320645c1f2e7 Mon Sep 17 00:00:00 2001
From: Phil Edworthy <phil.edworthy@renesas.com>
Date: Mon, 18 Apr 2016 15:12:48 +0100
Subject: [PATCH] spi-dw: Add DMA support

The DMA code is largely based on the Intel Midfield DMA code. The
major difference is that transmitted data is seen at the receive
side, and we need to transmit when receiving data.

Signed-off-by: Phil Edworthy <phil.edworthy@renesas.com>
---
 .../devicetree/bindings/spi/snps,dw-apb-ssi.txt    |   5 +
 drivers/spi/spi-dw-mmio.c                          | 366 +++++++++++++++++++++
 drivers/spi/spi-dw.h                               |  11 +
 3 files changed, 382 insertions(+)

diff --git a/Documentation/devicetree/bindings/spi/snps,dw-apb-ssi.txt b/Documentation/devicetree/bindings/spi/snps,dw-apb-ssi.txt
index 204b311..3e13d87 100644
--- a/Documentation/devicetree/bindings/spi/snps,dw-apb-ssi.txt
+++ b/Documentation/devicetree/bindings/spi/snps,dw-apb-ssi.txt
@@ -12,6 +12,8 @@ Optional properties:
 - num-cs : The number of chipselects. If omitted, this will default to 4.
 - reg-io-width : The I/O register width (in bytes) implemented by this
   device.  Supported values are 2 or 4 (the default).
+- dmas : Two dma phandles, one for rx, one for tx
+- dma-names : Must be "rx", "tx"
 
 Child nodes as per the generic SPI binding.
 
@@ -26,5 +28,8 @@ Example:
 		num-cs = <2>;
 		cs-gpios = <&gpio0 13 0>,
 			   <&gpio0 14 0>;
+		dmas =  <&dma0 8 0 0>,
+			<&dma0 9 0 0>;
+		dma-names = "rx", "tx";
 	};
 
diff --git a/drivers/spi/spi-dw-mmio.c b/drivers/spi/spi-dw-mmio.c
index 3da06c8..6159376 100644
--- a/drivers/spi/spi-dw-mmio.c
+++ b/drivers/spi/spi-dw-mmio.c
@@ -3,12 +3,17 @@
  *
  * Copyright (c) 2010, Octasic semiconductor.
  *
+ * DMA parts copied from spi-dw-mid.c:
+ * Copyright (c) 2009, 2014 Intel Corporation.
+ *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
  * version 2, as published by the Free Software Foundation.
  */
 
 #include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dmaengine.h>
 #include <linux/err.h>
 #include <linux/interrupt.h>
 #include <linux/platform_device.h>
@@ -20,6 +25,7 @@
 #include <linux/of_gpio.h>
 #include <linux/of_platform.h>
 #include <linux/property.h>
+#include <linux/platform_data/dma-dw.h>
 
 #include "spi-dw.h"
 
@@ -28,6 +34,350 @@
 struct dw_spi_mmio {
 	struct dw_spi  dws;
 	struct clk     *clk;
+	struct device  *dw_spi_dev;
+	struct delayed_work	work;
+	struct workqueue_struct *work_queue;
+};
+
+#define RX_BUSY		0
+#define TX_BUSY		1
+
+static int mmio_spi_dma_init(struct dw_spi *dws)
+{
+	struct dw_spi_mmio *dwsmmio = dws->priv;
+	struct device *dev = dwsmmio->dw_spi_dev;
+	int ret;
+
+	/* RX */
+	dws->rxchan = dma_request_slave_channel_reason(dev, "rx");
+	if (IS_ERR(dws->rxchan))
+		return PTR_ERR(dws->rxchan);
+
+	dev_info(dev, "DMA channel RX %s\n", dma_chan_name(dws->rxchan));
+	dws->master->dma_rx = dws->rxchan;
+
+	/* TX */
+	dws->txchan = dma_request_slave_channel_reason(dev, "tx");
+	if (IS_ERR(dws->txchan)) {
+		ret = PTR_ERR(dws->txchan);
+		goto err_rx;
+	}
+
+	dev_info(dev, "DMA channel TX %s\n", dma_chan_name(dws->txchan));
+	dws->master->dma_tx = dws->txchan;
+
+	dws->dma_inited = 1;
+	return 0;
+
+err_rx:
+	dma_release_channel(dws->rxchan);
+	return ret;
+}
+
+static void mmio_spi_dma_exit(struct dw_spi *dws)
+{
+	if (!dws->dma_inited)
+		return;
+
+	dmaengine_terminate_all(dws->txchan);
+	dma_release_channel(dws->txchan);
+
+	dmaengine_terminate_all(dws->rxchan);
+	dma_release_channel(dws->rxchan);
+}
+
+static irqreturn_t dma_transfer(struct dw_spi *dws)
+{
+	u16 irq_status = dw_readl(dws, DW_SPI_ISR);
+
+	if (!irq_status)
+		return IRQ_NONE;
+
+	dw_readl(dws, DW_SPI_ICR);
+	spi_reset_chip(dws);
+
+	dev_err(&dws->master->dev, "%s: FIFO overrun/underrun\n", __func__);
+	if (irq_status & SPI_INT_TXEI)
+		dev_err(&dws->master->dev, "%s: Tx FIFO underrun\n", __func__);
+	if (irq_status & SPI_INT_TXOI)
+		dev_err(&dws->master->dev, "%s: Tx FIFO overrun\n", __func__);
+	if (irq_status & SPI_INT_RXUI)
+		dev_err(&dws->master->dev, "%s: Rx FIFO underrun\n", __func__);
+	if (irq_status & SPI_INT_RXOI)
+		dev_err(&dws->master->dev, "%s: Rx FIFO overrun\n", __func__);
+	if (irq_status & SPI_INT_RXFI)
+		dev_err(&dws->master->dev, "%s: Rx FIFO full\n", __func__);
+	dws->master->cur_msg->status = -EIO;
+	spi_finalize_current_transfer(dws->master);
+	return IRQ_HANDLED;
+}
+
+static bool mmio_spi_can_dma(struct spi_master *master, struct spi_device *spi,
+		struct spi_transfer *xfer)
+{
+	struct dw_spi *dws = spi_master_get_devdata(master);
+
+	if (!dws->dma_inited)
+		return false;
+
+	return xfer->len > dws->fifo_len;
+}
+
+static enum dma_slave_buswidth convert_dma_width(u32 dma_width)
+{
+	if (dma_width == 1)
+		return DMA_SLAVE_BUSWIDTH_1_BYTE;
+	else if (dma_width == 2)
+		return DMA_SLAVE_BUSWIDTH_2_BYTES;
+
+	return DMA_SLAVE_BUSWIDTH_UNDEFINED;
+}
+
+static void dma_tx_complete_work(struct work_struct *work)
+{
+	struct dw_spi_mmio *dwsmmio =
+		container_of(work, struct dw_spi_mmio, work.work);
+	struct dw_spi *dws = &dwsmmio->dws;
+
+	spi_finalize_current_transfer(dws->master);
+}
+
+/*
+ * dws->dma_chan_busy is set before the dma transfer starts, callback for tx
+ * channel will clear a corresponding bit.
+ */
+static void dw_spi_dma_tx_done(void *arg)
+{
+	struct dw_spi *dws = arg;
+	struct dw_spi_mmio *dwsmmio = dws->priv;
+	u32 delay;
+	u32 words_to_tx;
+
+	clear_bit(TX_BUSY, &dws->dma_chan_busy);
+	if (test_bit(RX_BUSY, &dws->dma_chan_busy))
+		return;
+
+	/*
+	 * Although the DMA is complete, the SPI transaction on the wire is not!
+	 * DMA writes to the SPI FIFO and so we need to wait until the FIFO has
+	 * been drained and shifted out.
+	 */
+	words_to_tx = dw_readl(dws, DW_SPI_TXFLR);
+
+	/* Calc time until the data has been put on the wire (in us) */
+	delay = 1000000 / (dws->max_freq / dw_readl(dws, DW_SPI_BAUDR));
+	delay *= words_to_tx;
+	delay *= (8 * dws->dma_width) + 2; /* 8 bits per byte + 2 for framing */
+
+	/*
+	 * At 500KHz SPI clock, this delay is 320us and we are in atomic
+	 * context. Therefore, we defer this to a workqueue.
+	 * Arbitary threshold where it is better to just delay.
+	 */
+	if (delay <= 20) {
+		udelay(delay);
+		spi_finalize_current_transfer(dws->master);
+	} else {
+		queue_delayed_work(dwsmmio->work_queue,
+				&dwsmmio->work, usecs_to_jiffies(delay) + 1);
+	}
+}
+
+static struct dma_async_tx_descriptor *dw_spi_dma_prepare_tx(struct dw_spi *dws,
+		struct spi_transfer *xfer)
+{
+	struct dma_slave_config txconf;
+	struct dma_async_tx_descriptor *txdesc;
+	u32 val = 0;
+
+	if (!xfer->tx_buf)
+		return NULL;
+
+	txconf.direction = DMA_MEM_TO_DEV;
+	txconf.dst_addr = dws->dma_addr;
+	txconf.dst_maxburst = dws->fifo_len / 2;
+	txconf.src_maxburst = 4;
+	txconf.dst_addr_width = convert_dma_width(dws->dma_width);
+	txconf.device_fc = false;
+
+#ifdef CONFIG_SPI_DW_RZN1
+	txconf.device_fc = true;
+
+	/* The DMAC uses the maxburst size if the transfer is bigger than it */
+	if (txconf.dst_maxburst == 1)
+		val |= SPI_xDMACR_1_WORD_BURST;
+	else if (txconf.dst_maxburst == 4)
+		val |= SPI_xDMACR_4_WORD_BURST;
+	else if (txconf.dst_maxburst == 8)
+		val |= SPI_xDMACR_8_WORD_BURST;
+	dw_writel(dws, DW_SPI_TDMACR, val);
+
+	/* Block size */
+	val |= (xfer->len / dws->dma_width) << SPI_xDMACR_BLK_SIZE_OFFSET;
+
+	val |= SPI_xDMACR_DMA_EN;
+	dw_writel(dws, DW_SPI_TDMACR, val);
+#endif
+
+	dmaengine_slave_config(dws->txchan, &txconf);
+
+	txdesc = dmaengine_prep_slave_sg(dws->txchan,
+				xfer->tx_sg.sgl,
+				xfer->tx_sg.nents,
+				DMA_MEM_TO_DEV,
+				DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!txdesc)
+		return NULL;
+
+	txdesc->callback = dw_spi_dma_tx_done;
+	txdesc->callback_param = dws;
+
+	return txdesc;
+}
+
+/*
+ * dws->dma_chan_busy is set before the dma transfer starts, callback for rx
+ * channel will clear a corresponding bit.
+ */
+static void dw_spi_dma_rx_done(void *arg)
+{
+	struct dw_spi *dws = arg;
+
+	clear_bit(RX_BUSY, &dws->dma_chan_busy);
+	if (test_bit(TX_BUSY, &dws->dma_chan_busy))
+		return;
+	spi_finalize_current_transfer(dws->master);
+}
+
+static struct dma_async_tx_descriptor *dw_spi_dma_prepare_rx(struct dw_spi *dws,
+		struct spi_transfer *xfer)
+{
+	struct dma_slave_config rxconf;
+	struct dma_async_tx_descriptor *rxdesc;
+	u32 val = 0;
+
+	if (!xfer->rx_buf)
+		return NULL;
+
+	rxconf.direction = DMA_DEV_TO_MEM;
+	rxconf.src_addr = dws->dma_addr;
+	rxconf.src_maxburst = dws->fifo_len / 2;
+	rxconf.dst_maxburst = 4;
+	rxconf.src_addr_width = convert_dma_width(dws->dma_width);
+	rxconf.device_fc = false;
+
+#ifdef CONFIG_SPI_DW_RZN1
+	rxconf.device_fc = true;
+
+	/* The DMAC uses the maxburst size if the transfer is bigger than it */
+	if (rxconf.src_maxburst == 1)
+		val |= SPI_xDMACR_1_WORD_BURST;
+	else if (rxconf.src_maxburst == 4)
+		val |= SPI_xDMACR_4_WORD_BURST;
+	else if (rxconf.src_maxburst == 8)
+		val |= SPI_xDMACR_8_WORD_BURST;
+	dw_writel(dws, DW_SPI_RDMACR, val);
+
+	/* Block size */
+	val |= (xfer->len / dws->dma_width) << SPI_xDMACR_BLK_SIZE_OFFSET;
+
+	val |= SPI_xDMACR_DMA_EN;
+	dw_writel(dws, DW_SPI_RDMACR, val);
+#endif
+
+	dmaengine_slave_config(dws->rxchan, &rxconf);
+
+	rxdesc = dmaengine_prep_slave_sg(dws->rxchan,
+				xfer->rx_sg.sgl,
+				xfer->rx_sg.nents,
+				DMA_DEV_TO_MEM,
+				DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!rxdesc)
+		return NULL;
+
+	rxdesc->callback = dw_spi_dma_rx_done;
+	rxdesc->callback_param = dws;
+
+	return rxdesc;
+}
+
+static int mmio_spi_dma_setup(struct dw_spi *dws, struct spi_transfer *xfer)
+{
+	u16 dma_ctrl = 0;
+	u32 cr0;
+
+	/* watermarks for initiating DMA requests, both set to half the FIFO */
+	dw_writel(dws, DW_SPI_DMARDLR, (dws->fifo_len / 2) - 1);
+	dw_writel(dws, DW_SPI_DMATDLR, dws->fifo_len / 2);
+
+	/* Always enable tx */
+	dma_ctrl |= SPI_DMA_TDMAE;
+	if (xfer->rx_buf)
+		dma_ctrl |= SPI_DMA_RDMAE;
+	dw_writel(dws, DW_SPI_DMACR, dma_ctrl);
+
+	/* If tx only, set transfer mode to avoid data in the rx fifo */
+	cr0 = dw_readl(dws, DW_SPI_CTRL0);
+	cr0 &= ~SPI_TMOD_MASK;
+	if (!xfer->rx_buf)
+		cr0 |= (SPI_TMOD_TO << SPI_TMOD_OFFSET);
+	else
+		cr0 |= (SPI_TMOD_TR << SPI_TMOD_OFFSET);
+	dw_writel(dws, DW_SPI_CTRL0, cr0);
+
+	/* Set the interrupt mask */
+	spi_umask_intr(dws, SPI_INT_TXOI | SPI_INT_RXUI | SPI_INT_RXOI);
+
+	dws->transfer_handler = dma_transfer;
+
+	return 0;
+}
+
+static int mmio_spi_dma_transfer(struct dw_spi *dws, struct spi_transfer *xfer)
+{
+	struct dma_async_tx_descriptor *txdesc, *rxdesc;
+
+	/* Prepare the TX dma transfer */
+	txdesc = dw_spi_dma_prepare_tx(dws, xfer);
+
+	/* Prepare the RX dma transfer */
+	rxdesc = dw_spi_dma_prepare_rx(dws, xfer);
+
+	/* rx must be started before tx due to spi instinct */
+	if (rxdesc) {
+		set_bit(RX_BUSY, &dws->dma_chan_busy);
+		dmaengine_submit(rxdesc);
+		dma_async_issue_pending(dws->rxchan);
+	}
+
+	if (txdesc) {
+		set_bit(TX_BUSY, &dws->dma_chan_busy);
+		dmaengine_submit(txdesc);
+		dma_async_issue_pending(dws->txchan);
+	}
+
+	return 0;
+}
+
+static void mmio_spi_dma_stop(struct dw_spi *dws)
+{
+	if (test_bit(TX_BUSY, &dws->dma_chan_busy)) {
+		dmaengine_terminate_all(dws->txchan);
+		clear_bit(TX_BUSY, &dws->dma_chan_busy);
+	}
+	if (test_bit(RX_BUSY, &dws->dma_chan_busy)) {
+		dmaengine_terminate_all(dws->rxchan);
+		clear_bit(RX_BUSY, &dws->dma_chan_busy);
+	}
+}
+
+static struct dw_spi_dma_ops mmio_dma_ops = {
+	.dma_init	= mmio_spi_dma_init,
+	.dma_exit	= mmio_spi_dma_exit,
+	.dma_setup	= mmio_spi_dma_setup,
+	.can_dma	= mmio_spi_can_dma,
+	.dma_transfer	= mmio_spi_dma_transfer,
+	.dma_stop	= mmio_spi_dma_stop,
 };
 
 static int dw_spi_mmio_probe(struct platform_device *pdev)
@@ -126,10 +476,26 @@ static int dw_spi_mmio_probe(struct platform_device *pdev)
 		}
 	}
 
+	dwsmmio->dw_spi_dev = &pdev->dev;
+	dws->priv = dwsmmio;
+	dws->paddr = (unsigned long)(mem->start);
+	dws->dma_ops = &mmio_dma_ops;
+
+	INIT_DELAYED_WORK(&dwsmmio->work, dma_tx_complete_work);
+	dwsmmio->work_queue = create_singlethread_workqueue(dev_name(&pdev->dev));
+	if (!dwsmmio->work_queue) {
+		dev_err(&pdev->dev, "failed to create workqueue\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
 	ret = dw_spi_add_host(&pdev->dev, dws);
 	if (ret)
 		goto out;
 
+	/* DMA requires tx whilst doing an rx */
+	dws->master->flags = SPI_MASTER_MUST_TX;
+
 	platform_set_drvdata(pdev, dwsmmio);
 	return 0;
 
diff --git a/drivers/spi/spi-dw.h b/drivers/spi/spi-dw.h
index 94dba02..0542c0d 100644
--- a/drivers/spi/spi-dw.h
+++ b/drivers/spi/spi-dw.h
@@ -35,6 +35,10 @@
 #define DW_SPI_VERSION			0x5c
 #define DW_SPI_DR			0x60
 
+/* RZ/N1 special registers */
+#define DW_SPI_TDMACR			0x100
+#define DW_SPI_RDMACR			0x104
+
 /* Bit fields in CTRLR0 */
 #define SPI_DFS_OFFSET			0
 
@@ -81,6 +85,13 @@
 #define SPI_DMA_RDMAE			(1 << 0)
 #define SPI_DMA_TDMAE			(1 << 1)
 
+/* RZ/N1 special registers */
+#define SPI_xDMACR_BLK_SIZE_OFFSET	3
+#define SPI_xDMACR_DMA_EN		(1 << 0)
+#define SPI_xDMACR_1_WORD_BURST		(0 << 1)
+#define SPI_xDMACR_4_WORD_BURST		(1 << 1)
+#define SPI_xDMACR_8_WORD_BURST		(2 << 1)
+
 /* TX RX interrupt level threshold, max can be 256 */
 #define SPI_INT_THRESHOLD		32
 
-- 
2.7.4

